{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('my_env': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0eaaa3d9f5c3dfde9489338827632b9ddb7eb01e5876f95260908cf2d1371f48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Oscars Acceptance Speeches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.4.1-py2.py3-none-any.whl (239 kB)\n",
      "\u001b[K     |████████████████████████████████| 239 kB 2.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyOpenSSL>=16.2.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (4.7.1)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (4.5.2)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (2.9.2)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/lorena/anaconda3/lib/python3.8/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /home/lorena/anaconda3/lib/python3.8/site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (4.5.2)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 8.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 3.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (2.9.2)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.5.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-18.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyasn1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in /home/lorena/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (2.9.2)\n",
      "Requirement already satisfied: pyasn1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Collecting Twisted>=17.9.0\n",
      "  Downloading Twisted-20.3.0.tar.bz2 (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 6.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: zope.interface>=4.1.3 in /home/lorena/anaconda3/lib/python3.8/site-packages (from scrapy) (4.7.1)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: attrs>=16.0.0 in /home/lorena/anaconda3/lib/python3.8/site-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 2.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: idna>=2.5 in /home/lorena/anaconda3/lib/python3.8/site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
      "Collecting incremental>=16.10.1\n",
      "  Using cached incremental-17.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting PyHamcrest!=1.10.0,>=1.9.0\n",
      "  Downloading PyHamcrest-2.0.2-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 486 kB/s \n",
      "\u001b[?25hCollecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/lorena/anaconda3/lib/python3.8/site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /home/lorena/anaconda3/lib/python3.8/site-packages (from zope.interface>=4.1.3->scrapy) (49.6.0)\n",
      "Building wheels for collected packages: protego, PyDispatcher, Twisted\n",
      "  Building wheel for protego (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=7765 sha256=62cc939990b4d50148ff41b1b3a5fdd02042c9c7a4f0d07edf321b5cf32326c8\n",
      "  Stored in directory: /home/lorena/.cache/pip/wheels/91/64/36/bd0d11306cb22a78c7f53d603c7eb74ebb6c211703bc40b686\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11515 sha256=c5cc3905555b0f9f5986064a04a7211f01a27f138ab3cc1e294a6dcf534a7a55\n",
      "  Stored in directory: /home/lorena/.cache/pip/wheels/3c/31/7f/d7d7b5f0b9bad841ed856138ff0c5ee2bf2e04dbeb413097c8\n",
      "  Building wheel for Twisted (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Twisted: filename=Twisted-20.3.0-cp38-cp38-linux_x86_64.whl size=3081534 sha256=af47e790affac7a4627ef1e7dadc7fa927d4d782ecc21692ac09a6cbdc24f80a\n",
      "  Stored in directory: /home/lorena/.cache/pip/wheels/f2/36/1b/99fe6d339e1559e421556c69ad7bc8c869145e86a756c403f4\n",
      "Successfully built protego PyDispatcher Twisted\n",
      "Installing collected packages: w3lib, cssselect, PyHamcrest, parsel, itemadapter, incremental, hyperlink, constantly, Automat, Twisted, service-identity, queuelib, PyDispatcher, protego, itemloaders, scrapy\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 PyHamcrest-2.0.2 Twisted-20.3.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-21.0.0 incremental-17.5.0 itemadapter-0.2.0 itemloaders-1.0.4 parsel-1.6.0 protego-0.1.16 queuelib-1.5.0 scrapy-2.4.1 service-identity-18.1.0 w3lib-1.22.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/lorena/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://aaspeechesdb.oscars.org/results.aspx?AC=PREV_RECORD&XC=/results.aspx&BU=http%3A%2F%2Faaspeechesdb.oscars.org%2F&TN=aatrans&SN=AUTO24082&SE=786&RN=1&MR=0&TR=0&TX=1000&ES=0&CS=0&XP=&RF=WebReportList&EF=&DF=WebReportOscars&RL=0&EL=0&DL=0&NP=255&ID=&MF=oscarsmsg.ini&MQ=&TI=0&DT=&ST=0&IR=0&NR=0&NB=0&SV=0&SS=0&BG=&FG=&QS=&OEX=ISO-8859-1&OEH=utf-8\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'headers': \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:81.0) Gecko/20100101 Firefox/81.0\"}\n",
    "BASE_URL = \"http://aaspeechesdb.oscars.org/results.aspx?AC=PREV_RECORD&XC=/results.aspx&BU=http%3A%2F%2Faaspeechesdb.oscars.org%2F&TN=aatrans&SN=AUTO24082&SE=786&RN=1&MR=0&TR=0&TX=1000&ES=0&CS=0&XP=&RF=WebReportList&EF=&DF=WebReportOscars&RL=0&EL=0&DL=0&NP=255&ID=&MF=oscarsmsg.ini&MQ=&TI=0&DT=&ST=0&IR=0&NR=0&NB=0&SV=0&SS=0&BG=&FG=&QS=&OEX=ISO-8859-1&OEH=utf-8\"\n",
    "\n",
    "songs_urls = []\n",
    "\n",
    "\n",
    "def get_artist_link(artist):\n",
    "    for artist in artist_input:\n",
    "        artist_soup = BeautifulSoup(\n",
    "            requests.get(\n",
    "                BASE_URL).text, \n",
    "                'html.parser')\n",
    "\n",
    "        for td in artist_soup.find_all('td'):\n",
    "            for a in td.find_all('a'):\n",
    "                songs_urls.append(a['href'])\n",
    "        \n",
    "        #pd.DataFrame(songs_urls).to_csv(f'data/{artist}_songurls.csv', sep='\\t', index=False)\n",
    "        \n",
    "        time.sleep(2)"
   ]
  }
 ]
}